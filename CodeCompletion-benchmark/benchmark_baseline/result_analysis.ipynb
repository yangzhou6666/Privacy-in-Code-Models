{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import statistics\n",
    "import Levenshtein\n",
    "import plyj.parser as plyj\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tree_sitter import Language, Parser\n",
    "from cliffs_delta import cliffs_delta as cd\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu, wilcoxon\n",
    "\n",
    "percentage = 20\n",
    "file_id = f\"gpt2/{percentage}/test_CodeGPT-small-java_checkpoint-epoch-5_victim.json\"\n",
    "# Path of the file containing perpexity and zlib scores\n",
    "pz_path = \"/mnt/hdd1/XXXX/LM_Memorization/\" + file_id\n",
    "# Path of the file containing 'id', 'input', 'gt', 'prediction', 'label', 'predicition_label' for each datapoint in a json format\n",
    "input_path = f\"/mnt/hdd1/XXXX/XXXX/Privacy-in-Code-Models/Classifier/classifier_save/PTM3/javaCorpus/CodeGPT-small-java/{percentage}/30/res_20_30.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 12980\n",
      "Keys: dict_keys(['id', 'input', 'gt', 'prediction', 'label', 'predicition_label', 'perplexity', 'zlib', 'perplexity_compare', 'zlib_compare'])\n",
      "Number of samples with predicition_label 0: 6401\n",
      "Number of samples with predicition_label 1: 6579\n"
     ]
    }
   ],
   "source": [
    "# Load files\n",
    "data_dict = {}\n",
    "with open(input_path, \"r\") as file:\n",
    "    for line_no, line in enumerate(file.readlines()):\n",
    "        json_line = json.loads(line)\n",
    "        if line_no in data_dict:\n",
    "            print(\"Line number already exists:\", line_no)\n",
    "        data_dict[line_no] = json_line\n",
    "\n",
    "with open(pz_path, \"r\") as file:\n",
    "    for line_no, line in enumerate(file.readlines()):\n",
    "        json_line = json.loads(line)\n",
    "\n",
    "        # Check if the line_no exists in data_dict, and append perplexity and zlib values\n",
    "        if line_no in data_dict:\n",
    "            data_dict[line_no][\"perplexity\"] = json_line[\"perplexity\"]\n",
    "            data_dict[line_no][\"zlib\"] = json_line[\"zlib\"]\n",
    "            data_dict[line_no][\"perplexity_compare\"] = json_line[\"perplexity_compare\"]\n",
    "            data_dict[line_no][\"zlib_compare\"] = json_line[\"zlib_compare\"]\n",
    "        else:\n",
    "            print(\"Line number not found:\", line_no)\n",
    "\n",
    "\n",
    "print (\"Number of samples:\", len(data_dict))\n",
    "#print (f\"The first sample: {data_dict[0]}\")\n",
    "print(f\"Keys: {data_dict[0].keys()}\")\n",
    "data_dict_pred_label_0 = {k: v for k, v in data_dict.items() if v[\"predicition_label\"] == 0}\n",
    "data_dict_pred_label_1 = {k: v for k, v in data_dict.items() if v[\"predicition_label\"] == 1}\n",
    "print(f\"Number of samples with predicition_label 0: {len(data_dict_pred_label_0)}\")\n",
    "print(f\"Number of samples with predicition_label 1: {len(data_dict_pred_label_1)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, ground_truths):\n",
    "    tp = sum(1 for p, gt in zip(predictions, ground_truths) if p == gt == 1)\n",
    "    tn = sum(1 for p, gt in zip(predictions, ground_truths) if p == gt == 0)\n",
    "    fp = sum(1 for p, gt in zip(predictions, ground_truths) if p == 1 and gt == 0)\n",
    "    fn = sum(1 for p, gt in zip(predictions, ground_truths) if p == 0 and gt == 1)\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0\n",
    "    tpr = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    fpr = fp / (fp + tn) if fp + tn > 0 else 0\n",
    "\n",
    "    return precision, recall, f1, accuracy, tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity - 10%:\n",
      "  Precision: 0.5466\n",
      "  Recall: 0.1094\n",
      "  F1: 0.1823\n",
      "  Accuracy: 0.5093\n",
      "\n",
      "perplexity - 20%:\n",
      "  Precision: 0.5818\n",
      "  Recall: 0.2328\n",
      "  F1: 0.3326\n",
      "  Accuracy: 0.5327\n",
      "\n",
      "perplexity - 30%:\n",
      "  Precision: 0.5884\n",
      "  Recall: 0.3532\n",
      "  F1: 0.4414\n",
      "  Accuracy: 0.5531\n",
      "\n",
      "perplexity - 40%:\n",
      "  Precision: 0.5846\n",
      "  Recall: 0.4678\n",
      "  F1: 0.5197\n",
      "  Accuracy: 0.5677\n",
      "\n",
      "perplexity - 50%:\n",
      "  Precision: 0.5763\n",
      "  Recall: 0.5764\n",
      "  F1: 0.5764\n",
      "  Accuracy: 0.5763\n",
      "\n",
      "perplexity - 60%:\n",
      "  Precision: 0.5628\n",
      "  Recall: 0.6755\n",
      "  F1: 0.6140\n",
      "  Accuracy: 0.5754\n",
      "\n",
      "zlib - 10%:\n",
      "  Precision: 0.2479\n",
      "  Recall: 0.0496\n",
      "  F1: 0.0827\n",
      "  Accuracy: 0.4495\n",
      "\n",
      "zlib - 20%:\n",
      "  Precision: 0.3400\n",
      "  Recall: 0.1361\n",
      "  F1: 0.1943\n",
      "  Accuracy: 0.4360\n",
      "\n",
      "zlib - 30%:\n",
      "  Precision: 0.3892\n",
      "  Recall: 0.2336\n",
      "  F1: 0.2920\n",
      "  Accuracy: 0.4335\n",
      "\n",
      "zlib - 40%:\n",
      "  Precision: 0.4204\n",
      "  Recall: 0.3364\n",
      "  F1: 0.3737\n",
      "  Accuracy: 0.4363\n",
      "\n",
      "zlib - 50%:\n",
      "  Precision: 0.4522\n",
      "  Recall: 0.4522\n",
      "  F1: 0.4522\n",
      "  Accuracy: 0.4522\n",
      "\n",
      "zlib - 60%:\n",
      "  Precision: 0.4723\n",
      "  Recall: 0.5669\n",
      "  F1: 0.5153\n",
      "  Accuracy: 0.4668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inference():\n",
    "    data = data_dict.values()\n",
    "    # Calculate precision, recall, f1, accuracy for different thresholds\n",
    "    for metric_name in [\"perplexity\", \"zlib\"]:\n",
    "        if metric_name == \"perplexity\":\n",
    "            values = sorted([entry[metric_name] for entry in data])\n",
    "        elif metric_name == \"zlib\":\n",
    "            values = sorted([entry[metric_name] for entry in data], reverse=True)\n",
    "\n",
    "        thresholds = {f\"{k}%\": values[int(len(values) * k / 100)] for k in range(10, 70, 10)}\n",
    "        ground_truths = [entry[\"label\"] for entry in data]\n",
    "\n",
    "        for threshold_name, threshold in thresholds.items():\n",
    "            if metric_name == \"perplexity\":\n",
    "                predictions = [1 if entry[metric_name] <= threshold else 0 for entry in data]\n",
    "            elif metric_name == \"zlib\":\n",
    "                predictions = [1 if entry[metric_name] >= threshold else 0 for entry in data]\n",
    "            precision, recall, f1, accuracy, tpr, fpr = calculate_metrics(predictions, ground_truths)\n",
    "\n",
    "            print(f\"{metric_name} - {threshold_name}:\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "            print(f\"  F1: {f1:.4f}\")\n",
    "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "            print()\n",
    "inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_result(title, data_list):\n",
    "    # Calculate the statistics\n",
    "    mean = statistics.mean(data_list)\n",
    "    median = statistics.median(data_list)\n",
    "    min_tokens = min(data_list)\n",
    "    max_tokens = max(data_list)\n",
    "    stdev = statistics.stdev(data_list) if len(data_list) > 1 else 0\n",
    "\n",
    "    # Print the statistics\n",
    "    print(title)\n",
    "    print(f\"- Number of samples: {len(data_list)}\")\n",
    "    print(f\"- Mean: {mean}\")\n",
    "    print(f\"- Median: {median}\")\n",
    "    print(f\"- Minimum: {min_tokens}\")\n",
    "    print(f\"- Maximum: {max_tokens}\")\n",
    "    print(f\"- Standard deviation: {stdev}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cliffs_delta(x,y): # cohen_d\n",
    "    number = abs(np.mean(x) - np.mean(y)) / np.sqrt((np.std(x, ddof=1) ** 2 + np.std(y, ddof=1) ** 2) / 2.0)\n",
    "    size = \"\"\n",
    "    if number < 0.2:\n",
    "        size =  'negligible'\n",
    "    elif number < 0.5:\n",
    "        size =  'small'\n",
    "    elif number < 0.8:\n",
    "        size =  'medium'\n",
    "    elif number < 1.3:\n",
    "        size =  'large'\n",
    "    else:\n",
    "        size =  'very large'\n",
    "    return number,size\n",
    "\n",
    "def delta_alt(x, y):\n",
    "    number = cd(x, y)\n",
    "    return number\n",
    "\n",
    "def stat_testing(list_1, list_2):\n",
    "\n",
    "    # Perform a two-sample t-test\n",
    "    result = stats.ranksums(list_1, list_2)\n",
    "    delta_1 = cliffs_delta(list_1, list_2)\n",
    "    delta_2 = delta_alt(list_1, list_2)\n",
    "    \n",
    "    stat, p_value = mannwhitneyu(list_1, list_2)\n",
    "    rbc = 1 - (2 * stat) / (len(list_1) * len(list_2))\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f\"T-statistic: {result.statistic}\")\n",
    "    print(f\"P-value: {result.pvalue}\")\n",
    "    print(f\"Cohens-Delta: {delta_1[0]} - {delta_1[1]}\")\n",
    "    print(f\"Cliffs-Delta: {delta_2[0]} - {delta_2[1]}\")\n",
    "    print(f\"Mann-Whitney U test Statistic: {stat:.2f}\")\n",
    "    print(f\"Mann-Whitney U test P-value: {p_value:.2e}\")\n",
    "    print(f\"Rank-biserial correlation: {rbc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity of the samples (ALL)\n",
      "- Number of samples: 12980\n",
      "- Mean: 18.137249623480123\n",
      "- Median: 4.944484233856201\n",
      "- Minimum: 1.1290384531021118\n",
      "- Maximum: 9417.759765625\n",
      "- Standard deviation: 105.69412363862935\n",
      "\n",
      "perplexity of the samples (Prediction Label = 1)\n",
      "- Number of samples: 6579\n",
      "- Mean: 8.756882474349613\n",
      "- Median: 4.397513389587402\n",
      "- Minimum: 1.3402657508850098\n",
      "- Maximum: 1182.55859375\n",
      "- Standard deviation: 26.192915778653763\n",
      "\n",
      "perplexity of the samples (Prediction Label = 0)\n",
      "- Number of samples: 6401\n",
      "- Mean: 27.778467476023415\n",
      "- Median: 5.769089698791504\n",
      "- Minimum: 1.1290384531021118\n",
      "- Maximum: 9417.759765625\n",
      "- Standard deviation: 147.53430568275073\n",
      "\n",
      "T-statistic: -23.63328694913563\n",
      "P-value: 1.753494107935673e-123\n",
      "Cohens-Delta: 0.17952706301948257 - negligible\n",
      "Cliffs-Delta: -0.2395596770235993 - small\n",
      "Mann-Whitney U test Statistic: 16011899.50\n",
      "Mann-Whitney U test P-value: 1.75e-123\n",
      "Rank-biserial correlation: 0.24\n",
      "\n",
      "zlib of the samples (ALL)\n",
      "- Number of samples: 12980\n",
      "- Mean: 0.44283757776552946\n",
      "- Median: 0.3700951582076483\n",
      "- Minimum: 0.052581261950286805\n",
      "- Maximum: 1.4705882352941178\n",
      "- Standard deviation: 0.22150150218824072\n",
      "\n",
      "zlib of the samples (Prediction Label = 1)\n",
      "- Number of samples: 6579\n",
      "- Mean: 0.4006432190419499\n",
      "- Median: 0.3501070663811563\n",
      "- Minimum: 0.09223465581553068\n",
      "- Maximum: 1.3636363636363635\n",
      "- Standard deviation: 0.17596180963062702\n",
      "\n",
      "zlib of the samples (Prediction Label = 0)\n",
      "- Number of samples: 6401\n",
      "- Mean: 0.48620528375559824\n",
      "- Median: 0.4011299435028249\n",
      "- Minimum: 0.052581261950286805\n",
      "- Maximum: 1.4705882352941178\n",
      "- Standard deviation: 0.25290936025114497\n",
      "\n",
      "T-statistic: -16.97476311670947\n",
      "P-value: 1.2626270996370766e-64\n",
      "Cohens-Delta: 0.39273928302389927 - small\n",
      "Cliffs-Delta: -0.17206530680827511 - small\n",
      "Mann-Whitney U test Statistic: 17433067.00\n",
      "Mann-Whitney U test P-value: 1.26e-64\n",
      "Rank-biserial correlation: 0.17\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stat 0: Perplexity and Zlib of the samples by pred_label\n",
    "for metric in [\"perplexity\", \"zlib\"]:\n",
    "    metric_list = [entry[metric] for entry in data_dict.values()]\n",
    "    category_title = f\"{metric} of the samples\"\n",
    "    stat_result(category_title + \" (ALL)\", metric_list)\n",
    "    metric_list_pred_label_0 = [entry[metric] for entry in data_dict_pred_label_0.values()]\n",
    "    metric_list_pred_label_1 = [entry[metric] for entry in data_dict_pred_label_1.values()]\n",
    "    stat_result(category_title + \" (Prediction Label = 1)\", metric_list_pred_label_1)\n",
    "    stat_result(category_title + \" (Prediction Label = 0)\", metric_list_pred_label_0)\n",
    "    stat_testing(metric_list_pred_label_1, metric_list_pred_label_0)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity of the samples (ALL)\n",
      "- Number of samples: 12980\n",
      "- Mean: 18.137249623480123\n",
      "- Median: 4.944484233856201\n",
      "- Minimum: 1.1290384531021118\n",
      "- Maximum: 9417.759765625\n",
      "- Standard deviation: 105.69412363862935\n",
      "\n",
      "perplexity of the samples (Label = 1)\n",
      "- Number of samples: 6490\n",
      "- Mean: 9.190490091728687\n",
      "- Median: 4.469345331192017\n",
      "- Minimum: 1.3402657508850098\n",
      "- Maximum: 1182.55859375\n",
      "- Standard deviation: 29.606217568890383\n",
      "\n",
      "perplexity of the samples (Label = 0)\n",
      "- Number of samples: 6490\n",
      "- Mean: 27.08400915523156\n",
      "- Median: 5.591537714004517\n",
      "- Minimum: 1.1290384531021118\n",
      "- Maximum: 9417.759765625\n",
      "- Standard deviation: 145.9711454657945\n",
      "\n",
      "T-statistic: -20.86329377647766\n",
      "P-value: 1.1544087371872662e-96\n",
      "Cohens-Delta: 0.1698986067597524 - negligible\n",
      "Cliffs-Delta: -0.21146165369977754 - small\n",
      "Mann-Whitney U test Statistic: 16606657.00\n",
      "Mann-Whitney U test P-value: 1.15e-96\n",
      "Rank-biserial correlation: 0.21\n",
      "\n",
      "zlib of the samples (ALL)\n",
      "- Number of samples: 12980\n",
      "- Mean: 0.44283757776552946\n",
      "- Median: 0.3700951582076483\n",
      "- Minimum: 0.052581261950286805\n",
      "- Maximum: 1.4705882352941178\n",
      "- Standard deviation: 0.22150150218824072\n",
      "\n",
      "zlib of the samples (Label = 1)\n",
      "- Number of samples: 6490\n",
      "- Mean: 0.4043562269890652\n",
      "- Median: 0.35373765924863965\n",
      "- Minimum: 0.09223465581553068\n",
      "- Maximum: 1.3478260869565217\n",
      "- Standard deviation: 0.1764292470684417\n",
      "\n",
      "zlib of the samples (Label = 0)\n",
      "- Number of samples: 6490\n",
      "- Mean: 0.4813189285419937\n",
      "- Median: 0.3943081372737811\n",
      "- Minimum: 0.052581261950286805\n",
      "- Maximum: 1.4705882352941178\n",
      "- Standard deviation: 0.25306921491105555\n",
      "\n",
      "T-statistic: -13.95084699142473\n",
      "P-value: 3.108912405071588e-44\n",
      "Cohens-Delta: 0.3528111367809091 - small\n",
      "Cliffs-Delta: -0.14139997293453718 - negligible\n",
      "Mann-Whitney U test Statistic: 18082159.50\n",
      "Mann-Whitney U test P-value: 3.11e-44\n",
      "Rank-biserial correlation: 0.14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stat 0.2: Perplexity and Zlib of the samples by label\n",
    "for metric in [\"perplexity\", \"zlib\"]:\n",
    "    metric_list = [entry[metric] for entry in data_dict.values()]\n",
    "    category_title = f\"{metric} of the samples\"\n",
    "    stat_result(category_title + \" (ALL)\", metric_list)\n",
    "    metric_list_label_0 = [entry[metric] for entry in data_dict.values() if entry[\"label\"] == 0]\n",
    "    metric_list_label_1 = [entry[metric] for entry in data_dict.values() if entry[\"label\"] == 1]\n",
    "    stat_result(category_title + \" (Label = 1)\", metric_list_label_1)\n",
    "    stat_result(category_title + \" (Label = 0)\", metric_list_label_0)\n",
    "    stat_testing(metric_list_label_1, metric_list_label_0)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in an example input (ALL)\n",
      "- Number of samples: 12980\n",
      "- Mean: 305.9798151001541\n",
      "- Median: 200.0\n",
      "- Minimum: 5\n",
      "- Maximum: 1199\n",
      "- Standard deviation: 293.4212796680094\n",
      "\n",
      "Number of tokens in an example input (Prediction Label = 1)\n",
      "- Number of samples: 6579\n",
      "- Mean: 326.07204742362063\n",
      "- Median: 230\n",
      "- Minimum: 5\n",
      "- Maximum: 1198\n",
      "- Standard deviation: 288.2441455088303\n",
      "\n",
      "Number of tokens in an example input (Prediction Label = 0)\n",
      "- Number of samples: 6401\n",
      "- Mean: 285.3288548664271\n",
      "- Median: 162\n",
      "- Minimum: 5\n",
      "- Maximum: 1199\n",
      "- Standard deviation: 297.2593072450871\n",
      "\n",
      "T-statistic: 13.545474644914854\n",
      "P-value: 8.42674442460592e-42\n",
      "Cohens-Delta: 0.13915669874076791 - negligible\n",
      "Cliffs-Delta: 0.1373041988637064 - negligible\n",
      "Mann-Whitney U test Statistic: 23947179.00\n",
      "Mann-Whitney U test P-value: 8.42e-42\n",
      "Rank-biserial correlation: -0.14\n"
     ]
    }
   ],
   "source": [
    "# Stat 1: The number of tokens in an example input\n",
    "num_tokenized_inputs = [len(entry[\"input\"].split()) for entry in data_dict.values()]\n",
    "category_title = \"Number of tokens in an example input\"\n",
    "stat_result(category_title + \" (ALL)\", num_tokenized_inputs)\n",
    "num_tokenized_inputs_predicition_label_1 = [len(entry[\"input\"].split()) for entry in data_dict.values() if entry[\"predicition_label\"] == 1]\n",
    "stat_result(category_title + \" (Prediction Label = 1)\", num_tokenized_inputs_predicition_label_1)\n",
    "num_tokenized_inputs_predicition_label_0 = [len(entry[\"input\"].split()) for entry in data_dict.values() if entry[\"predicition_label\"] == 0]\n",
    "stat_result(category_title + \" (Prediction Label = 0)\", num_tokenized_inputs_predicition_label_0)\n",
    "stat_testing(num_tokenized_inputs_predicition_label_1, num_tokenized_inputs_predicition_label_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in an example output (ALL)\n",
      "- Number of samples: 12980\n",
      "- Mean: 5.873882896764253\n",
      "- Median: 6.0\n",
      "- Minimum: 0\n",
      "- Maximum: 75\n",
      "- Standard deviation: 3.431166849371259\n",
      "\n",
      "Number of tokens in an example output (Prediction Label = 1)\n",
      "- Number of samples: 6579\n",
      "- Mean: 6.063231494148047\n",
      "- Median: 6\n",
      "- Minimum: 0\n",
      "- Maximum: 68\n",
      "- Standard deviation: 3.0440591473884657\n",
      "\n",
      "Number of tokens in an example output (Prediction Label = 0)\n",
      "- Number of samples: 6401\n",
      "- Mean: 5.679268864239963\n",
      "- Median: 5\n",
      "- Minimum: 0\n",
      "- Maximum: 75\n",
      "- Standard deviation: 3.778403659663294\n",
      "\n",
      "T-statistic: 9.104608083213428\n",
      "P-value: 8.657845537803609e-20\n",
      "Cohens-Delta: 0.11191195023362839 - negligible\n",
      "Cliffs-Delta: 0.09228919263474825 - negligible\n",
      "Mann-Whitney U test Statistic: 22999339.00\n",
      "Mann-Whitney U test P-value: 4.32e-20\n",
      "Rank-biserial correlation: -0.09\n"
     ]
    }
   ],
   "source": [
    "# Stat 2: The number of tokens in an example output\n",
    "num_tokenized_outputs = [len(entry[\"prediction\"].split()) for entry in data_dict.values()]\n",
    "category_title = \"Number of tokens in an example output\"\n",
    "stat_result(category_title + \" (ALL)\", num_tokenized_outputs)\n",
    "num_tokenized_outputs_predicition_label_1 = [len(entry[\"prediction\"].split()) for entry in data_dict.values() if entry[\"predicition_label\"] == 1]\n",
    "stat_result(category_title + \" (Prediction Label = 1)\", num_tokenized_outputs_predicition_label_1)\n",
    "num_tokenized_outputs_predicition_label_0 = [len(entry[\"prediction\"].split()) for entry in data_dict.values() if entry[\"predicition_label\"] == 0]\n",
    "stat_result(category_title + \" (Prediction Label = 0)\", num_tokenized_outputs_predicition_label_0)\n",
    "stat_testing(num_tokenized_outputs_predicition_label_1, num_tokenized_outputs_predicition_label_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance between the victim model output and the ground truth (ALL)\n",
      "- Number of samples: 12980\n",
      "- Mean: 17.502927580893683\n",
      "- Median: 13.0\n",
      "- Minimum: 1\n",
      "- Maximum: 1722\n",
      "- Standard deviation: 23.76555964331625\n",
      "\n",
      "Edit distance between the victim model output and the ground truth (Prediction Label = 1)\n",
      "- Number of samples: 6579\n",
      "- Mean: 16.209454324365407\n",
      "- Median: 11\n",
      "- Minimum: 1\n",
      "- Maximum: 186\n",
      "- Standard deviation: 17.911344507791\n",
      "\n",
      "Edit distance between the victim model output and the ground truth (Prediction Label = 0)\n",
      "- Number of samples: 6401\n",
      "- Mean: 18.832369942196532\n",
      "- Median: 15\n",
      "- Minimum: 1\n",
      "- Maximum: 1722\n",
      "- Standard deviation: 28.498665355120078\n",
      "\n",
      "T-statistic: -9.648956728735286\n",
      "P-value: 4.9658339826938e-22\n",
      "Cohens-Delta: 0.11020117935881027 - negligible\n",
      "Cliffs-Delta: -0.09780700257756789 - negligible\n",
      "Mann-Whitney U test Statistic: 18996656.50\n",
      "Mann-Whitney U test P-value: 1.29e-22\n",
      "Rank-biserial correlation: 0.10\n"
     ]
    }
   ],
   "source": [
    "# Stat 3: The edit distance between the victim model output and the ground truth.\n",
    "outputs = [entry[\"prediction\"] for entry in data_dict.values()]\n",
    "gts = [entry[\"gt\"] for entry in data_dict.values()]\n",
    "edit_distances = [Levenshtein.distance(model_output, ground_truth) for model_output, ground_truth in zip(outputs, gts)]\n",
    "category_title = \"Edit distance between the victim model output and the ground truth\"\n",
    "stat_result(category_title + \" (ALL)\", edit_distances)\n",
    "outputs_predicition_label_1 = [entry[\"prediction\"] for entry in data_dict.values() if entry[\"predicition_label\"] == 1]\n",
    "gts_predicition_label_1 = [entry[\"gt\"] for entry in data_dict.values() if entry[\"predicition_label\"] == 1]\n",
    "outputs_predicition_label_0 = [entry[\"prediction\"] for entry in data_dict.values() if entry[\"predicition_label\"] == 0]\n",
    "gts_predicition_label_0 = [entry[\"gt\"] for entry in data_dict.values() if entry[\"predicition_label\"] == 0]\n",
    "edit_distances_predicition_label_1 = [Levenshtein.distance(model_output, ground_truth) for model_output, ground_truth in zip(outputs_predicition_label_1, gts_predicition_label_1)]\n",
    "edit_distances_predicition_label_0 = [Levenshtein.distance(model_output, ground_truth) for model_output, ground_truth in zip(outputs_predicition_label_0, gts_predicition_label_0)]\n",
    "stat_result(category_title + \" (Prediction Label = 1)\", edit_distances_predicition_label_1)\n",
    "stat_result(category_title + \" (Prediction Label = 0)\", edit_distances_predicition_label_0)\n",
    "stat_testing(edit_distances_predicition_label_1, edit_distances_predicition_label_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variable names in the example (input + gt) (ALL)\n",
      "- Number of samples: 12980\n",
      "- Mean: 6.244144838212635\n",
      "- Median: 3.0\n",
      "- Minimum: 0\n",
      "- Maximum: 205\n",
      "- Standard deviation: 8.670587176147551\n",
      "\n",
      "Number of variable names in the example (input + gt) (Prediction Label = 1)\n",
      "- Number of samples: 6579\n",
      "- Mean: 6.664994680042559\n",
      "- Median: 4\n",
      "- Minimum: 0\n",
      "- Maximum: 154\n",
      "- Standard deviation: 8.629652968043496\n",
      "\n",
      "Number of variable names in the example (input + gt) (Prediction Label = 0)\n",
      "- Number of samples: 6401\n",
      "- Mean: 5.811591938759569\n",
      "- Median: 3\n",
      "- Minimum: 0\n",
      "- Maximum: 205\n",
      "- Standard deviation: 8.691921355831518\n",
      "\n",
      "T-statistic: 8.3280420910604\n",
      "P-value: 8.219029443623229e-17\n",
      "Cohens-Delta: 0.09853575791135999 - negligible\n",
      "Cliffs-Delta: 0.08441750306959894 - negligible\n",
      "Mann-Whitney U test Statistic: 22833592.00\n",
      "Mann-Whitney U test P-value: 4.93e-17\n",
      "Rank-biserial correlation: -0.08\n"
     ]
    }
   ],
   "source": [
    "# Stat 4: The number of variable names in the example (input + gt)\n",
    "# You need to install tree-sitter and tree-sitter-java and set the path to the language.so file\n",
    "path = '/mnt/hdd1/XXXX/LM_Memorization/python_parser/parser_folder/my-languages.so'\n",
    "parser = Parser()\n",
    "parser.set_language(Language(path, \"java\"))\n",
    "\n",
    "def num_var(code_snippet):\n",
    "    tree = parser.parse(bytes(code_snippet, \"utf8\"))\n",
    "    def count_variable_declarations(node):\n",
    "        variable_count = 0\n",
    "        if node.type == 'variable_declarator':\n",
    "            variable_count += 1\n",
    "\n",
    "        for child in node.children:\n",
    "            variable_count += count_variable_declarations(child)\n",
    "\n",
    "        return variable_count\n",
    "\n",
    "    root_node = tree.root_node\n",
    "    variable_count = count_variable_declarations(root_node)\n",
    "    return variable_count\n",
    "\n",
    "num_var_all = [num_var(entry[\"input\"] + entry[\"gt\"]) for entry in data_dict.values()]\n",
    "num_var_predicition_label_1 = [num_var(entry[\"input\"] + entry[\"gt\"]) for entry in data_dict.values() if entry[\"predicition_label\"] == 1]\n",
    "num_var_predicition_label_0 = [num_var(entry[\"input\"] + entry[\"gt\"]) for entry in data_dict.values() if entry[\"predicition_label\"] == 0]\n",
    "category_title = \"Number of variable names in the example (input + gt)\"\n",
    "stat_result(category_title + \" (ALL)\", num_var_all)\n",
    "stat_result(category_title + \" (Prediction Label = 1)\", num_var_predicition_label_1)\n",
    "stat_result(category_title + \" (Prediction Label = 0)\", num_var_predicition_label_0)\n",
    "stat_testing(num_var_predicition_label_1, num_var_predicition_label_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity - 10%:\n",
      "  Precision: 0.5466\n",
      "  Recall: 0.1094\n",
      "  F1: 0.1823\n",
      "  Accuracy: 0.5093\n",
      "  Power(TPR): 0.1094\n",
      "  Error(FPR): 0.0908\n",
      "  AUC: 0.5093\n",
      "\n",
      "perplexity - 20%:\n",
      "  Precision: 0.5818\n",
      "  Recall: 0.2328\n",
      "  F1: 0.3326\n",
      "  Accuracy: 0.5327\n",
      "  Power(TPR): 0.2328\n",
      "  Error(FPR): 0.1673\n",
      "  AUC: 0.5327\n",
      "\n",
      "perplexity - 30%:\n",
      "  Precision: 0.5884\n",
      "  Recall: 0.3532\n",
      "  F1: 0.4414\n",
      "  Accuracy: 0.5531\n",
      "  Power(TPR): 0.3532\n",
      "  Error(FPR): 0.2470\n",
      "  AUC: 0.5531\n",
      "\n",
      "perplexity - 40%:\n",
      "  Precision: 0.5846\n",
      "  Recall: 0.4678\n",
      "  F1: 0.5197\n",
      "  Accuracy: 0.5677\n",
      "  Power(TPR): 0.4678\n",
      "  Error(FPR): 0.3324\n",
      "  AUC: 0.5677\n",
      "\n",
      "perplexity - 50%:\n",
      "  Precision: 0.5763\n",
      "  Recall: 0.5764\n",
      "  F1: 0.5764\n",
      "  Accuracy: 0.5763\n",
      "  Power(TPR): 0.5764\n",
      "  Error(FPR): 0.4237\n",
      "  AUC: 0.5763\n",
      "\n",
      "perplexity - 60%:\n",
      "  Precision: 0.5628\n",
      "  Recall: 0.6755\n",
      "  F1: 0.6140\n",
      "  Accuracy: 0.5754\n",
      "  Power(TPR): 0.6755\n",
      "  Error(FPR): 0.5247\n",
      "  AUC: 0.5754\n",
      "\n",
      "perplexity - 70%:\n",
      "  Precision: 0.5563\n",
      "  Recall: 0.7789\n",
      "  F1: 0.6490\n",
      "  Accuracy: 0.5788\n",
      "  Power(TPR): 0.7789\n",
      "  Error(FPR): 0.6213\n",
      "  AUC: 0.5788\n",
      "\n",
      "perplexity_compare - 10%:\n",
      "  Precision: 0.4296\n",
      "  Recall: 0.0860\n",
      "  F1: 0.1433\n",
      "  Accuracy: 0.4859\n",
      "  Power(TPR): 0.0860\n",
      "  Error(FPR): 0.1142\n",
      "  AUC: 0.4859\n",
      "\n",
      "perplexity_compare - 20%:\n",
      "  Precision: 0.4336\n",
      "  Recall: 0.1735\n",
      "  F1: 0.2478\n",
      "  Accuracy: 0.4734\n",
      "  Power(TPR): 0.1735\n",
      "  Error(FPR): 0.2267\n",
      "  AUC: 0.4734\n",
      "\n",
      "perplexity_compare - 30%:\n",
      "  Precision: 0.4503\n",
      "  Recall: 0.2703\n",
      "  F1: 0.3378\n",
      "  Accuracy: 0.4702\n",
      "  Power(TPR): 0.2703\n",
      "  Error(FPR): 0.3299\n",
      "  AUC: 0.4702\n",
      "\n",
      "perplexity_compare - 40%:\n",
      "  Precision: 0.4602\n",
      "  Recall: 0.3683\n",
      "  F1: 0.4091\n",
      "  Accuracy: 0.4682\n",
      "  Power(TPR): 0.3683\n",
      "  Error(FPR): 0.4319\n",
      "  AUC: 0.4682\n",
      "\n",
      "perplexity_compare - 50%:\n",
      "  Precision: 0.4727\n",
      "  Recall: 0.4727\n",
      "  F1: 0.4727\n",
      "  Accuracy: 0.4727\n",
      "  Power(TPR): 0.4727\n",
      "  Error(FPR): 0.5274\n",
      "  AUC: 0.4727\n",
      "\n",
      "perplexity_compare - 60%:\n",
      "  Precision: 0.4813\n",
      "  Recall: 0.5777\n",
      "  F1: 0.5251\n",
      "  Accuracy: 0.4776\n",
      "  Power(TPR): 0.5777\n",
      "  Error(FPR): 0.6225\n",
      "  AUC: 0.4776\n",
      "\n",
      "perplexity_compare - 70%:\n",
      "  Precision: 0.4931\n",
      "  Recall: 0.6904\n",
      "  F1: 0.5753\n",
      "  Accuracy: 0.4904\n",
      "  Power(TPR): 0.6904\n",
      "  Error(FPR): 0.7097\n",
      "  AUC: 0.4904\n",
      "\n",
      "zlib_compare - 10%:\n",
      "  Precision: 0.5527\n",
      "  Recall: 0.1106\n",
      "  F1: 0.1844\n",
      "  Accuracy: 0.5106\n",
      "  Power(TPR): 0.1106\n",
      "  Error(FPR): 0.0895\n",
      "  AUC: 0.5106\n",
      "\n",
      "zlib_compare - 20%:\n",
      "  Precision: 0.5518\n",
      "  Recall: 0.2208\n",
      "  F1: 0.3154\n",
      "  Accuracy: 0.5207\n",
      "  Power(TPR): 0.2208\n",
      "  Error(FPR): 0.1794\n",
      "  AUC: 0.5207\n",
      "\n",
      "zlib_compare - 30%:\n",
      "  Precision: 0.5556\n",
      "  Recall: 0.3334\n",
      "  F1: 0.4168\n",
      "  Accuracy: 0.5334\n",
      "  Power(TPR): 0.3334\n",
      "  Error(FPR): 0.2667\n",
      "  AUC: 0.5334\n",
      "\n",
      "zlib_compare - 40%:\n",
      "  Precision: 0.5517\n",
      "  Recall: 0.4414\n",
      "  F1: 0.4905\n",
      "  Accuracy: 0.5414\n",
      "  Power(TPR): 0.4414\n",
      "  Error(FPR): 0.3587\n",
      "  AUC: 0.5414\n",
      "\n",
      "zlib_compare - 50%:\n",
      "  Precision: 0.5466\n",
      "  Recall: 0.5467\n",
      "  F1: 0.5466\n",
      "  Accuracy: 0.5466\n",
      "  Power(TPR): 0.5467\n",
      "  Error(FPR): 0.4535\n",
      "  AUC: 0.5466\n",
      "\n",
      "zlib_compare - 60%:\n",
      "  Precision: 0.5399\n",
      "  Recall: 0.6479\n",
      "  F1: 0.5890\n",
      "  Accuracy: 0.5478\n",
      "  Power(TPR): 0.6479\n",
      "  Error(FPR): 0.5522\n",
      "  AUC: 0.5478\n",
      "\n",
      "zlib_compare - 70%:\n",
      "  Precision: 0.5305\n",
      "  Recall: 0.7428\n",
      "  F1: 0.6190\n",
      "  Accuracy: 0.5428\n",
      "  Power(TPR): 0.7428\n",
      "  Error(FPR): 0.6573\n",
      "  AUC: 0.5428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# Stat 5: Power Error and AUC of K = 50% prediction based on Perplexity, Compare Perplexity and Compare Zlib\n",
    "def inference_2():\n",
    "    data = data_dict.values()\n",
    "    # Calculate precision, recall, f1, accuracy for different thresholds\n",
    "    for metric_name in [\"perplexity\", \"perplexity_compare\",\"zlib_compare\"]:\n",
    "        \n",
    "        values = sorted([entry[metric_name] for entry in data])\n",
    "        thresholds = {f\"{k}%\": values[int(len(values) * k / 100)] for k in range(10, 71, 10)}\n",
    "        ground_truths = [entry[\"label\"] for entry in data]\n",
    "\n",
    "        for threshold_name, threshold in thresholds.items():\n",
    "            predictions = [1 if entry[metric_name] <= threshold else 0 for entry in data]\n",
    "            precision, recall, f1, accuracy, tpr, fpr = calculate_metrics(predictions, ground_truths)\n",
    "            auc = roc_auc_score(ground_truths, predictions)\n",
    "\n",
    "            print(f\"{metric_name} - {threshold_name}:\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "            print(f\"  F1: {f1:.4f}\")\n",
    "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Power(TPR): {tpr:.4f}\")\n",
    "            print(f\"  Error(FPR): {fpr:.4f}\")\n",
    "            print(f\"  AUC: {auc:.4f}\")\n",
    "            print()\n",
    "inference_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
      "|    |       Metric       | Threshold |      Precision      |       Recall        |         F1          |      Accuracy       |     Power (TPR)     |     Error (FPR)     |         AUC         |\n",
      "+----+--------------------+-----------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+\n",
      "| 0  |     perplexity     |    10%    | 0.5465742879137798  | 0.10939907550077041 | 0.1823083836179227  | 0.5093220338983051  | 0.10939907550077041 | 0.09075500770416024 |  0.509322033898305  |\n",
      "| 1  |     perplexity     |    20%    | 0.5818251829033501  | 0.23281972265023113 | 0.3325630020908991  | 0.5327426810477658  | 0.23281972265023113 | 0.16733436055469955 | 0.5327426810477659  |\n",
      "| 2  |     perplexity     |    30%    | 0.5884467265725288  | 0.3531587057010786  | 0.44140587385652386 | 0.5530816640986133  | 0.3531587057010786  | 0.2469953775038521  | 0.5530816640986133  |\n",
      "| 3  |     perplexity     |    40%    | 0.5846331600231081  | 0.46779661016949153 | 0.5197295215270051  | 0.5677195685670262  | 0.46779661016949153 | 0.3323574730354391  | 0.5677195685670262  |\n",
      "| 4  |     perplexity     |    50%    | 0.5763364658758281  | 0.5764252696456086  | 0.5763808643401895  | 0.5763482280431433  | 0.5764252696456086  |  0.423728813559322  | 0.5763482280431432  |\n",
      "| 5  |     perplexity     |    60%    | 0.5628450378739248  | 0.6755007704160246  | 0.6140486028433364  | 0.5754237288135593  | 0.6755007704160246  |  0.524653312788906  | 0.5754237288135593  |\n",
      "| 6  |     perplexity     |    70%    | 0.5562892043578739  | 0.7788906009244992  | 0.6490338319316942  | 0.5788135593220339  | 0.7788906009244992  | 0.6212634822804314  | 0.5788135593220339  |\n",
      "| 7  | perplexity_compare |    10%    | 0.4295612009237875  | 0.08597842835130971 | 0.1432789831814097  | 0.48590138674884437 | 0.08597842835130971 | 0.11417565485362095 | 0.48590138674884437 |\n",
      "| 8  | perplexity_compare |    20%    | 0.43357720446669235 | 0.17349768875192603 | 0.24782656542313192 | 0.4734206471494607  | 0.17349768875192603 | 0.22665639445300462 | 0.47342064714946064 |\n",
      "| 9  | perplexity_compare |    30%    | 0.4503209242618742  | 0.2702619414483821  | 0.33779489648531535 | 0.47018489984591677 | 0.2702619414483821  | 0.3298921417565485  | 0.4701848998459167  |\n",
      "| 10 | perplexity_compare |    40%    | 0.46023493163874446 | 0.3682588597842835  | 0.4091414876316015  | 0.4681818181818182  | 0.3682588597842835  | 0.4318952234206471  | 0.4681818181818182  |\n",
      "| 11 | perplexity_compare |    50%    | 0.4726544446156216  | 0.4727272727272727  | 0.4726908558662661  | 0.4726502311248074  | 0.4727272727272727  | 0.5274268104776579  | 0.4726502311248074  |\n",
      "| 12 | perplexity_compare |    60%    | 0.48131980998844526 | 0.5776579352850539  | 0.5251068001960921  | 0.4775808936825886  | 0.5776579352850539  | 0.6224961479198767  | 0.4775808936825886  |\n",
      "| 13 | perplexity_compare |    70%    | 0.49312204247826563 | 0.6904468412942989  | 0.5753354304423187  | 0.4903697996918336  | 0.6904468412942989  | 0.7097072419106317  | 0.4903697996918336  |\n",
      "| 14 |    zlib_compare    |    10%    |  0.552732871439569  | 0.11063174114021572 | 0.18436256258826553 | 0.5105546995377503  | 0.11063174114021572 | 0.08952234206471495 | 0.5105546995377505  |\n",
      "| 15 |    zlib_compare    |    20%    | 0.5517905275317674  | 0.22080123266563945 | 0.3153956201166501  | 0.5207241910631741  | 0.22080123266563945 | 0.17935285053929123 | 0.5207241910631741  |\n",
      "| 16 |    zlib_compare    |    30%    |  0.555584082156611  | 0.33343605546995375 | 0.41675493500240735 | 0.5333590138674884  | 0.33343605546995375 | 0.2667180277349769  | 0.5333590138674884  |\n",
      "| 17 |    zlib_compare    |    40%    | 0.5517042172154824  | 0.44144838212634824 | 0.49045621843704523 | 0.5413713405238829  | 0.44144838212634824 | 0.35870570107858246 | 0.5413713405238828  |\n",
      "| 18 |    zlib_compare    |    50%    | 0.5466029887536589  | 0.5466872110939908  | 0.5466450966797627  | 0.5466101694915254  | 0.5466872110939908  | 0.4534668721109399  | 0.5466101694915255  |\n",
      "| 19 |    zlib_compare    |    60%    | 0.5398639106432148  | 0.6479198767334361  | 0.5889768191049793  | 0.5478428351309708  | 0.6479198767334361  | 0.5522342064714946  | 0.5478428351309708  |\n",
      "| 20 |    zlib_compare    |    70%    | 0.5305381313965005  | 0.7428351309707242  |  0.618989535854144  | 0.5427580893682589  | 0.7428351309707242  | 0.6573189522342064  | 0.5427580893682589  |\n",
      "+----+--------------------+-----------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "from tabulate import tabulate\n",
    "import zlib\n",
    "\n",
    "\n",
    "def inference_2_csv():\n",
    "    data = data_dict.values()\n",
    "    results = []\n",
    "\n",
    "    for metric_name in [\"perplexity\", \"perplexity_compare\", \"zlib_compare\"]:\n",
    "        values = sorted([entry[metric_name] for entry in data])\n",
    "        thresholds = {f\"{k}%\": values[int(len(values) * k / 100)] for k in range(10, 71, 10)}\n",
    "        ground_truths = [entry[\"label\"] for entry in data]\n",
    "\n",
    "        for threshold_name, threshold in thresholds.items():\n",
    "            predictions = [1 if entry[metric_name] <= threshold else 0 for entry in data]\n",
    "            precision, recall, f1, accuracy, tpr, fpr = calculate_metrics(predictions, ground_truths)\n",
    "            auc = roc_auc_score(ground_truths, predictions)\n",
    "\n",
    "            results.append({\n",
    "                \"Metric\": metric_name,\n",
    "                \"Threshold\": threshold_name,\n",
    "                \"Precision\": precision,\n",
    "                \"Recall\": recall,\n",
    "                \"F1\": f1,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"Power (TPR)\": tpr,\n",
    "                \"Error (FPR)\": fpr,\n",
    "                \"AUC\": auc\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(tabulate(df, headers='keys', tablefmt='pretty'))\n",
    "    df.to_csv('power.csv', index=False)\n",
    "\n",
    "inference_2_csv()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
